{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, we show you how to perform deep learning using a convolutional neural network to classify images of cats, dogs and pandas. We show you how to do a grid search using the TALC to find the optimal parameters for training. We then show you how to perform transfer learning on the same dataset. The dataset we use can be found at"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing libraries and defining classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from torch.optim.lr_scheduler import ExponentialLR\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms, models\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vision Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, file_paths, labels, transform=None):\n",
    "        self.file_paths = file_paths\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        label = self.labels[idx]\n",
    "        file_path = self.file_paths[idx]\n",
    "\n",
    "        image = Image.open(file_path)\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Custom Neural Network Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AnimalModel(nn.Module):\n",
    "    def __init__(self, num_classes, input_shape, transfer=False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.transfer = transfer\n",
    "        self.num_classes = num_classes\n",
    "        self.input_shape = input_shape\n",
    "\n",
    "        # transfer learning if pretrained=True\n",
    "        self.feature_extractor = models.densenet161(pretrained=transfer)\n",
    "\n",
    "        if self.transfer:\n",
    "            # layers are frozen by using eval()\n",
    "            self.feature_extractor.eval()\n",
    "            # freeze params\n",
    "            for param in self.feature_extractor.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "        n_features = self._get_conv_output(self.input_shape)\n",
    "        self.classifier = nn.Linear(n_features, num_classes)\n",
    "\n",
    "    def _get_conv_output(self, shape):\n",
    "        batch_size = 1\n",
    "        tmp_input = torch.autograd.Variable(torch.rand(batch_size, *shape))\n",
    "\n",
    "        output_feat = self.feature_extractor(tmp_input)\n",
    "        n_size = output_feat.data.view(batch_size, -1).size(1)\n",
    "        return n_size\n",
    "\n",
    "    # will be used during inference\n",
    "    def forward(self, x):\n",
    "        x = self.feature_extractor(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data loader helper class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_loaders(val_split, test_split, batch_size=32, verbose=True):\n",
    "    num_workers = 0\n",
    "    random_state = 10\n",
    "    n_splits = 1\n",
    "\n",
    "    # Listing the data\n",
    "    # Cats\n",
    "    print(\"LISTING DATA\")\n",
    "    input_dir = \"dataset/temp/cats\"\n",
    "    images = [os.path.join(input_dir, image) for image in os.listdir(input_dir)]\n",
    "    # There are several images that only have one channel, and the model uses three channels, so this code preprocesses the data to remove the 1-channel images\n",
    "    three_channels = []\n",
    "    for filename in images:\n",
    "        img = Image.open(filename)\n",
    "        if img.mode == 'RGB':\n",
    "            three_channels.append(filename)\n",
    "    cat_images = np.array(three_channels)  # transform to numpy\n",
    "    cat_labels = ['cat'] * len(cat_images)\n",
    "\n",
    "    # Dogs\n",
    "    input_dir2 = \"dataset/temp/dogs\"\n",
    "    images2 = [os.path.join(input_dir2, image) for image in os.listdir(input_dir2)]\n",
    "    # There are several images that only have one channel, and the model uses three channels, so this code preprocesses the data to remove the 1-channel images\n",
    "    three_channels = []\n",
    "    for filename in images2:\n",
    "        img = Image.open(filename)\n",
    "        if img.mode == 'RGB':\n",
    "            three_channels.append(filename)\n",
    "    dog_images = np.array(three_channels)  # transform to numpy\n",
    "    dog_labels = ['dog'] * len(dog_images)\n",
    "\n",
    "    # Panda\n",
    "    input_dir3 = \"dataset/temp/panda\"\n",
    "    images3 = [os.path.join(input_dir3, image) for image in os.listdir(input_dir3)]\n",
    "    # There are several images that only have one channel, and the model uses three channels, so this code preprocesses the data to remove the 1-channel images\n",
    "    three_channels = []\n",
    "    for filename in images3:\n",
    "        img = Image.open(filename)\n",
    "        if img.mode == 'RGB':\n",
    "            three_channels.append(filename)\n",
    "    panda_images = np.array(three_channels)  # transform to numpy\n",
    "    panda_labels = ['panda'] * len(panda_images)\n",
    "\n",
    "    # Appending lists\n",
    "    images = np.append(np.append(cat_images, dog_images), panda_images)\n",
    "    labels = cat_labels + dog_labels + panda_labels\n",
    "    labels = np.array(labels)\n",
    "\n",
    "    # Formatting the labs as ints\n",
    "    classes = np.unique(labels).flatten()\n",
    "    labels_int = np.zeros(labels.size, dtype=np.int64)\n",
    "\n",
    "    # Convert string labels to integers\n",
    "    for index, class_name in enumerate(classes):\n",
    "        labels_int[labels == class_name] = index\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Number of images in the dataset:\", len(images))\n",
    "        for index, class_name in enumerate(classes):\n",
    "            print(\"Number of images in class \", class_name,\n",
    "                  \":\", (labels_int == index).sum())\n",
    "\n",
    "    # Splitting the data in dev and test sets\n",
    "    sss = StratifiedShuffleSplit(\n",
    "        n_splits=n_splits, test_size=test_split, random_state=random_state)\n",
    "    sss.get_n_splits(images, labels_int)\n",
    "    dev_index, test_index = next(sss.split(images, labels_int))\n",
    "\n",
    "    dev_images = images[dev_index]\n",
    "    dev_labels = labels_int[dev_index]\n",
    "\n",
    "    test_images = images[test_index]\n",
    "    test_labels = labels_int[test_index]\n",
    "\n",
    "    # Splitting the data in train and val sets\n",
    "    val_size = int(val_split * images.size)\n",
    "    val_split = val_size / dev_images.size\n",
    "    sss2 = StratifiedShuffleSplit(\n",
    "        n_splits=n_splits, test_size=val_split, random_state=random_state)\n",
    "    sss2.get_n_splits(dev_images, dev_labels)\n",
    "    train_index, val_index = next(sss2.split(dev_images, dev_labels))\n",
    "\n",
    "    train_images = images[train_index]\n",
    "    train_labels = labels_int[train_index]\n",
    "\n",
    "    val_images = images[val_index]\n",
    "    val_labels = labels_int[val_index]\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Train set:\", train_images.size)\n",
    "        print(\"Val set:\", val_images.size)\n",
    "        print(\"Test set:\", test_images.size)\n",
    "\n",
    "    # Transforms\n",
    "    torchvision_transform_train = transforms.Compose(\n",
    "        [transforms.Resize((args.unified_image_width, args.unified_image_height)),\n",
    "         transforms.RandomHorizontalFlip(),\n",
    "         transforms.RandomVerticalFlip(),\n",
    "         transforms.ToTensor()])\n",
    "\n",
    "    # Datasets\n",
    "    train_dataset_unorm = CustomDataset(\n",
    "        train_images, train_labels, transform=torchvision_transform_train)\n",
    "\n",
    "    # Get training set stats\n",
    "    trainloader_unorm = torch.utils.data.DataLoader(\n",
    "        train_dataset_unorm, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "\n",
    "    mean_train, std_train = get_dataset_stats(trainloader_unorm)\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Statistics of training set\")\n",
    "        print(\"Mean:\", mean_train)\n",
    "        print(\"Std:\", std_train)\n",
    "\n",
    "    torchvision_transform = transforms.Compose(\n",
    "        [transforms.Resize((args.unified_image_width, args.unified_image_height)),\n",
    "         transforms.RandomHorizontalFlip(), transforms.RandomVerticalFlip(),\n",
    "         transforms.ToTensor(),\n",
    "         transforms.Normalize(mean=mean_train, std=std_train)])\n",
    "\n",
    "    torchvision_transform_test = transforms.Compose(\n",
    "        [transforms.Resize((args.unified_image_width, args.unified_image_height)),\n",
    "         transforms.ToTensor(),\n",
    "         transforms.Normalize(mean=mean_train, std=std_train)])\n",
    "\n",
    "    # Get the train/val/test loaders\n",
    "    train_dataset = CustomDataset(\n",
    "        train_images, train_labels, transform=torchvision_transform)\n",
    "    val_dataset = CustomDataset(val_images, val_labels, transform=torchvision_transform)\n",
    "    test_dataset = CustomDataset(\n",
    "        test_images, test_labels, transform=torchvision_transform_test)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "    val_loader = torch.utils.data.DataLoader(\n",
    "        val_dataset, batch_size=batch_size, num_workers=num_workers)\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        test_dataset, batch_size=batch_size, num_workers=num_workers)\n",
    "\n",
    "    return train_loader, val_loader, test_loader"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Statistics helper class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset_stats(data_loader):\n",
    "    mean = 0.\n",
    "    std = 0.\n",
    "    nb_samples = 0.\n",
    "    for data in data_loader:\n",
    "        samples = data[0]\n",
    "        batch_samples = samples.size(0)\n",
    "        samples = samples.view(batch_samples, samples.size(1), -1)\n",
    "        mean += samples.mean(2).sum(0)\n",
    "        std += samples.std(2).sum(0)\n",
    "        nb_samples += batch_samples\n",
    "\n",
    "    mean /= nb_samples\n",
    "    std /= nb_samples\n",
    "    return mean, std"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model training helper classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_hyperparameters_and_train_model(best_model_path, device, verbose, patience, epoch, batch, rate):\n",
    "    epochs_range = [epoch]\n",
    "    batch_size_range = [batch]\n",
    "    learning_rate_range = [rate]\n",
    "\n",
    "    # Perform the grid search\n",
    "    best_val_loss = float('inf')\n",
    "    best_hyperparameters = None\n",
    "\n",
    "    for epochs in epochs_range:\n",
    "        for batch_size in batch_size_range:\n",
    "            for learning_rate in learning_rate_range:\n",
    "                train_loader, val_loader, test_loader = get_data_loaders(args.val_split, args.test_split)\n",
    "                current_model = AnimalModel(args.num_classes,\n",
    "                                            (args.num_classes, args.unified_image_width, args.unified_image_height))\n",
    "                current_model.to(device)\n",
    "                val_loss = train_validate_with_hyperparameters(current_model, train_loader, val_loader, epochs,\n",
    "                                                               learning_rate, best_model_path, device, patience,\n",
    "                                                               verbose)\n",
    "                if val_loss < best_val_loss:\n",
    "                    best_val_loss = val_loss\n",
    "                    best_hyperparameters = (epochs, batch_size, learning_rate)\n",
    "                    torch.save(current_model.state_dict(), best_model_path)\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Best nbr epochs:\", best_hyperparameters[0])\n",
    "        print(\"Best batch size:\", best_hyperparameters[1])\n",
    "        print(\"Best learning rate:\", best_hyperparameters[2])\n",
    "\n",
    "    return best_hyperparameters\n",
    "\n",
    "def train_validate_with_hyperparameters(observed_model, train_loader, val_loader, epochs,\n",
    "                                        learning_rate, best_model_path, device, patience, verbose):\n",
    "    best_loss = float(\"inf\")\n",
    "    gamma = 0.9\n",
    "    counter = 0\n",
    "\n",
    "    # Loss function and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()  # Loss function\n",
    "    optimizer = torch.optim.AdamW(observed_model.parameters(), lr=learning_rate)\n",
    "    scheduler = ExponentialLR(optimizer, gamma=gamma)\n",
    "\n",
    "    for epoch in range(epochs):  # loop over the dataset multiple times\n",
    "\n",
    "        # Training Loop\n",
    "        train_loss = 0.0\n",
    "        for i, data in enumerate(train_loader, 0):\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            inputs, labels = data[0].to(device), data[1].to(device)\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = observed_model(inputs)\n",
    "\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "        print(f'{epoch + 1},  train loss: {train_loss / i:.3f},', end=' ')\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        val_loss = 0\n",
    "        # since we're not training, we don't need to calculate the gradients for our outputs\n",
    "        with torch.no_grad():\n",
    "            for i, data in enumerate(val_loader, 0):\n",
    "                # get the inputs; data is a list of [inputs, labels]\n",
    "                inputs, labels = data[0].to(device), data[1].to(device)\n",
    "                outputs = observed_model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                val_loss += loss.item()\n",
    "            print(f'val loss: {val_loss / i:.3f}')\n",
    "\n",
    "            # Save best model\n",
    "            if val_loss < best_loss:\n",
    "                print(\"Saving model\")\n",
    "                torch.save(observed_model.state_dict(), best_model_path)\n",
    "                best_loss = val_loss\n",
    "                counter = 0\n",
    "            else:\n",
    "                counter += 1\n",
    "                if counter >= patience:\n",
    "                    if verbose:\n",
    "                        print(f'Validation loss has not improved for {patience} epochs. Stopping training.')\n",
    "                    break\n",
    "\n",
    "    return val_loss / (i + 1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model testing helper class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(net, testloader, device):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    # since we're not training, we don't need to calculate the gradients for our outputs\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            images, labels = data[0].to(device), data[1].to(device)\n",
    "            # calculate outputs by running images through the network\n",
    "            outputs = net(images)\n",
    "            # the class with the highest energy is what we choose as prediction\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print(\n",
    "        f'Accuracy of the network on the test images: {100 * correct / total} %')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main program sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Constants definition\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--learning_rate', type=float, default=1e-4, help='initial learning rate')\n",
    "    parser.add_argument('--batch_size', type=int, default=2,\n",
    "                        help='batch size,  number of images in each iteration during training')\n",
    "    parser.add_argument('--epochs', type=int, default=10, help='total epochs')\n",
    "    parser.add_argument('--val_split', type=float, default=0.2, help='val split')\n",
    "    parser.add_argument('--test_split', type=float, default=0.2, help='test split')\n",
    "    parser.add_argument('--best_model_path', type=str, default=\"best_model\", help='best model path')\n",
    "    parser.add_argument('--verbose', type=bool, default=True, help='verbose debugging flag')\n",
    "    parser.add_argument('--transfer_learning', type=bool, default=True, help='transfer learning flag')\n",
    "    parser.add_argument('--num_classes', type=int, default=3, help='Number of classes in dataset')\n",
    "    parser.add_argument('--unified_image_height', type=int, default=224, help='transfer learning flag')\n",
    "    parser.add_argument('--unified_image_width', type=int, default=224, help='transfer learning flag')\n",
    "    parser.add_argument('--patience', type=int, default=5, help='transfer learning flag')\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    # Device choice\n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "    if args.verbose:\n",
    "        print(\"Chosen device:\", device)\n",
    "\n",
    "    # Model training and saving\n",
    "    best_batch_size = define_hyperparameters_and_train_model(args.best_model_path, device, args.verbose, args.patience, args.epochs, args.batch_size, args.learning_rate)[\n",
    "        1]\n",
    "\n",
    "    # Get test dataloader\n",
    "    test_loader = get_data_loaders(args.val_split, args.test_split, batch_size=best_batch_size, verbose=args.verbose)[2]\n",
    "\n",
    "    # Loading best model\n",
    "    model = AnimalModel(args.num_classes, (args.num_classes, args.unified_image_width, args.unified_image_height))\n",
    "    model.load_state_dict(torch.load(args.best_model_path))\n",
    "\n",
    "    # Best model testing\n",
    "    test(model, test_loader, device)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
